{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\32133\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "#Method is use BERT classifier\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#Firstly, use pandas to sepearte text and label\n",
                "df_train = pd.read_csv('train_data.txt', sep=\";\", header=None)\n",
                "df_val = pd.read_csv('val_data.txt', sep=\";\", header=None)\n",
                "#rename column to text and elabel\n",
                "df_train.columns=['text','elabel']\n",
                "df_val.columns=['text','elabel']\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
                        "c:\\Users\\32133\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2342: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
                        "  FutureWarning,\n"
                    ]
                }
            ],
            "source": [
                "#convert text to token by BertTokenizer\n",
                "from transformers import BertTokenizer\n",
                "\n",
                "#load berttokenizaer, 12-layer BERT model, with uncased vocabulary\n",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
                "\n",
                "input_ids_train, attention_masks_train = [], []\n",
                "\n",
                "#tokenize ['text'] in df_train \n",
                "for text in df_train['text']:\n",
                "    encoded_sent = tokenizer.encode_plus(\n",
                "            text=text,                      #text sentence \n",
                "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
                "            max_length=150,                  #max lenght sentense input =150\n",
                "            pad_to_max_length=True,         #Pad sentence to max length\n",
                "            return_attention_mask=True      #Return attention mask\n",
                "            )\n",
                "        \n",
                "    input_ids_train.append(encoded_sent.get('input_ids'))\n",
                "    attention_masks_train.append(encoded_sent.get('attention_mask'))\n",
                "\n",
                "#to tensor\n",
                "input_ids_train = torch.tensor(input_ids_train)\n",
                "attention_masks_train = torch.tensor(attention_masks_train)\n",
                "\n",
                "\n",
                "#tokenize ['text'] in df_val \n",
                "input_ids_val, attention_masks_val = [], []\n",
                "\n",
                "for text in df_val['text']:\n",
                "    encoded_sent = tokenizer.encode_plus(\n",
                "            text=text,                      #text sentence \n",
                "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
                "            max_length=150,                  #max lenght sentense input =150\n",
                "            pad_to_max_length=True,         #Pad sentence to max length\n",
                "            return_attention_mask=True      #Return attention mask\n",
                "            )\n",
                "\n",
                "    input_ids_val.append(encoded_sent.get('input_ids'))\n",
                "    attention_masks_val.append(encoded_sent.get('attention_mask'))\n",
                "    \n",
                "#to tensor\n",
                "input_ids_val = torch.tensor(input_ids_val)\n",
                "attention_masks_val = torch.tensor(attention_masks_val)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'anger': 0, 'fear': 1, 'joy': 2, 'love': 3, 'sadness': 4, 'surprise': 5}\n"
                    ]
                }
            ],
            "source": [
                "#convert elabel to tensor at test and val dataset, labelencoder assign the category by number set at list below, this list will use again \n",
                "#in gui to get what number is what emotion\n",
                "from sklearn import preprocessing\n",
                "le = preprocessing.LabelEncoder()\n",
                "le.fit(df_val['elabel'])\n",
                "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
                "print(le_name_mapping)\n",
                "val_label = torch.tensor(le.fit_transform(df_val['elabel'].values))\n",
                "train_label = torch.tensor(le.fit_transform(df_train['elabel'].values))\n",
                "#print(le.inverse_transform(val_label))\n",
                "#print(le.inverse_transform(train_label))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Use cuda device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "#set up cuda if available, elase use cpu\n",
                "\n",
                "if torch.cuda.is_available():       \n",
                "    device = torch.device(\"cuda\")\n",
                "    #deafult use gpu0\n",
                "    print(\"Use cuda device:\", torch.cuda.get_device_name(0))\n",
                "else:\n",
                "    device = torch.device(\"cpu\")\n",
                "    print(\"use cpu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size=16    #can run on 6gbvram\n",
                "\n",
                "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
                "\n",
                "#create dataloader for train and val\n",
                "train_data = TensorDataset(input_ids_train, attention_masks_train, train_label)\n",
                "train_sampler = RandomSampler(train_data)\n",
                "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
                "\n",
                "val_data = TensorDataset(input_ids_val, attention_masks_val, val_label)\n",
                "val_sampler = SequentialSampler(val_data)\n",
                "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create classifier by bert\n",
                "from transformers import BertModel\n",
                "\n",
                "class emotionClassifier(nn.Module):\n",
                "    def __init__(self):\n",
                "         super(emotionClassifier, self).__init__()\n",
                "         self.bert = BertModel.from_pretrained('bert-base-uncased')     #use bert-base-uncased\n",
                "\n",
                "        #Classifier use hidden neron fc 768 ->48 relu -> 6 (6 label)\n",
                "         self.classifier = nn.Sequential(\n",
                "            nn.Linear(768, 48),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(48, 6)\n",
                "        )\n",
                "    \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        # input to BERT\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        \n",
                "        # get [CLS] at last hidden layer output for classifier to use\n",
                "        cls_from_bert = outputs[0][:, 0, :]\n",
                "\n",
                "        # put cls_from_bert into our classifier\n",
                "        result = self.classifier(cls_from_bert)\n",
                "\n",
                "        return result\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Optimizatize the model\n",
                "\n",
                "from transformers import AdamW, get_linear_schedule_with_warmup\n",
                "\n",
                "def initialize_model(epochs):\n",
                "    #start classifier\n",
                "    bert_classifier = emotionClassifier()\n",
                "    bert_classifier.to(device)  #put classifier into device choosed before, gpu for my case\n",
                "    #use AdamW optimization\n",
                "    optimizer = AdamW(bert_classifier.parameters(),\n",
                "                    lr=5e-5,    # learning rate\n",
                "                    eps=1e-8    # epsilon value\n",
                "                    )\n",
                "\n",
                "    total_training_steps = len(train_dataloader) * epochs      #define training step\n",
                "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
                "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
                "                                            num_training_steps = total_training_steps)\n",
                "    \n",
                "    return bert_classifier, optimizer, scheduler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "loss_function = nn.CrossEntropyLoss()       #use crossentropyloss\n",
                "\n",
                "def train(model, train_dataloader, epochs):\n",
                "    print(\"training initialized\")\n",
                "    print(\"##############################\")\n",
                "    train_time_start = time.time()      #time for whole trining\n",
                "    \n",
                "    for i in range(epochs):\n",
                "        epochs_no = i+1\n",
                "        print(str(epochs_no)+\"/\"+str(epochs)+\"epochs\")\n",
                "        train_time_epochs_start = time.time()   #time for each epochs\n",
                "        total_train_loss = 0\n",
                "        \n",
                "        #train mode\n",
                "        model.train()\n",
                "        batch_no=0\n",
                "\n",
                "        accuracy_list_train = []\n",
                "\n",
                "        for batch in train_dataloader:\n",
                "            batch_no+=1\n",
                "            #batch to device\n",
                "            batch_input, batch_mask, batch_label = tuple(v.to(device) for v in batch)\n",
                "            model.zero_grad()       #zero gradident, reset it\n",
                "\n",
                "            #forward pass, return result\n",
                "            result = model(batch_input, batch_mask)\n",
                "\n",
                "            #calculate loss from result and label\n",
                "            batch_label = batch_label.to(torch.int64)     #to int64 or it crash\n",
                "            loss = loss_function(result, batch_label)\n",
                "            total_train_loss += loss.item()\n",
                "\n",
                "            #backward pass\n",
                "            loss.backward()\n",
                "            #clip norm to 1.0, prevent exploding gradients\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "\n",
                "            #update parameter\n",
                "            optimizer.step()\n",
                "            #update learning rate\n",
                "            scheduler.step()\n",
                "\n",
                "            classified_result = torch.argmax(result, dim=1).flatten()\n",
                "            #calculate the accuracy on batch train set\n",
                "            accuracy = (classified_result == batch_label).cpu().numpy().mean() * 100\n",
                "            accuracy_list_train.append(accuracy)\n",
                "\n",
                "            if (batch_no % 250 == 0 and batch_no != 0):     #each 250 batch report once time\n",
                "                print(\"batch number at:\"+str(batch_no))\n",
                "                time_used_in_batch = time.time()-train_time_start\n",
                "                print(\"@\"+str(time_used_in_batch)+\"s from start training\")\n",
                "        \n",
                "            \n",
                "        average_loss = total_train_loss/len(train_dataloader)\n",
                "        time_used = time.time()-train_time_epochs_start\n",
                "        print(\"//////////////////\")\n",
                "        print(\"Average loss (train_set) = \"+str(average_loss))\n",
                "        print(\"Performance on training dataset: Accuracy = \"+str(np.mean(accuracy_list_train)))\n",
                "        print(\"Time (this epochs) = \"+str(time_used))\n",
                "        print(\"//////////////////\")\n",
                "    \n",
                "    time_used = time.time()-train_time_start\n",
                "    print(\"Training complete, time used = \"+str(time_used))\n",
                "\n",
                "\n",
                "        \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def evaluate(model, val_dataloader):        #evalute the model\n",
                "    model.eval()        #evaluate mode\n",
                "\n",
                "    loss_list = []\n",
                "    accuracy_list = []\n",
                "    \n",
                "    for i in val_dataloader:\n",
                "        batch_input, batch_mask, batch_label = tuple(v.to(device) for v in i)\n",
                "\n",
                "        #get result\n",
                "        with torch.no_grad():\n",
                "            result = model(batch_input, batch_mask)\n",
                "\n",
                "        batch_label = batch_label.to(torch.int64)     #to int64 or it crash        \n",
                "\n",
                "        #get loss\n",
                "        loss  = loss_function(result, batch_label)\n",
                "        loss_list.append(loss.item())\n",
                "\n",
                "        #get classified label in number\n",
                "        classified_result = torch.argmax(result, dim=1).flatten()\n",
                "        #calculate the accuracy on batch val set\n",
                "        accuracy = (classified_result == batch_label).cpu().numpy().mean() * 100\n",
                "        accuracy_list.append(accuracy)\n",
                "\n",
                "    #calculate and print loss and accuracy on valid set\n",
                "    print(\"Performance on validation dataset: Accuracy = \"+str(np.mean(accuracy_list))+\", Loss = \"+str(np.mean(loss_list)))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
                        "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "c:\\Users\\32133\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
                        "  FutureWarning,\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "training initialized\n",
                        "##############################\n",
                        "1/4epochs\n",
                        "batch number at:250\n",
                        "@77.84821248054504s from start training\n",
                        "batch number at:500\n",
                        "@154.73544096946716s from start training\n",
                        "batch number at:750\n",
                        "@232.0023000240326s from start training\n",
                        "batch number at:1000\n",
                        "@309.53027153015137s from start training\n",
                        "//////////////////\n",
                        "Average loss (train_set) = 0.4546272170562297\n",
                        "Performance on training dataset: Accuracy = 86.0125\n",
                        "Time (this epochs) = 309.53027153015137\n",
                        "//////////////////\n",
                        "2/4epochs\n",
                        "batch number at:250\n",
                        "@387.10116839408875s from start training\n",
                        "batch number at:500\n",
                        "@465.68584275245667s from start training\n",
                        "batch number at:750\n",
                        "@543.1851859092712s from start training\n",
                        "batch number at:1000\n",
                        "@620.6781048774719s from start training\n",
                        "//////////////////\n",
                        "Average loss (train_set) = 0.14608690163097343\n",
                        "Performance on training dataset: Accuracy = 94.23125\n",
                        "Time (this epochs) = 311.1488342285156\n",
                        "//////////////////\n",
                        "3/4epochs\n",
                        "batch number at:250\n",
                        "@698.2098064422607s from start training\n",
                        "batch number at:500\n",
                        "@775.6254570484161s from start training\n",
                        "batch number at:750\n",
                        "@853.0521032810211s from start training\n",
                        "batch number at:1000\n",
                        "@930.4098520278931s from start training\n",
                        "//////////////////\n",
                        "Average loss (train_set) = 0.10236501161789056\n",
                        "Performance on training dataset: Accuracy = 95.5375\n",
                        "Time (this epochs) = 309.7317464351654\n",
                        "//////////////////\n",
                        "4/4epochs\n",
                        "batch number at:250\n",
                        "@1007.8599538803101s from start training\n",
                        "batch number at:500\n",
                        "@1085.2372298240662s from start training\n",
                        "batch number at:750\n",
                        "@1162.6647589206696s from start training\n",
                        "batch number at:1000\n",
                        "@1240.12655544281s from start training\n",
                        "//////////////////\n",
                        "Average loss (train_set) = 0.0682313768165186\n",
                        "Performance on training dataset: Accuracy = 97.28125\n",
                        "Time (this epochs) = 309.71570324897766\n",
                        "//////////////////\n",
                        "Training complete, time used = 1240.12655544281\n"
                    ]
                }
            ],
            "source": [
                "bert_classifier, optimizer, scheduler = initialize_model(epochs=4)     #initializa model\n",
                "train(bert_classifier, train_dataloader, epochs=4)                      #train model for 4 epochs\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "#creat checkpoint\n",
                "checkpoint_state = {'state_dict': bert_classifier.state_dict(), 'optimizer': optimizer.state_dict()}\n",
                "torch.save(checkpoint_state, 'checkpoint.pt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "#if need load the checkpoint, uncomment below three lines\n",
                "#loaded_checkpoint = torch.load('checkpoint.pt')\n",
                "#bert_classifier.load_state_dict(loaded_checkpoint['state_dict'])\n",
                "#optimizer.load_state_dict(loaded_checkpoint['optimizer'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Performance on validation dataset: Accuracy = 93.95, Loss = 0.211594626782462\n"
                    ]
                }
            ],
            "source": [
                "#evaluate by val_data\n",
                "evaluate(bert_classifier, val_dataloader)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "8a8ef75465bfab8bf494df63c7f3e7f7b0d48dfa24ac528a356eb6497e842cef"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
